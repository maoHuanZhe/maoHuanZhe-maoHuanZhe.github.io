---
title: 使用分析API来分析文本
date: 2022-04-19 20:31:46
permalink: /pages/7ea1a2/
categories:
  - Elasticsearch
  - 分析数据
tags:
  - 
author: 
  name: 樊光瑞
  link: https://github.com/maoHuanZhe
---

当跟踪信息是如何在 `Elasticsearch` 索引中存储的时候，使用分析 `API` 来测试分析的过程是十分有用的。这个 `API` 允许你向 `Elasticsearch` 发送任何文本，指定所使用的分析器、分词器或者分词过滤器，然后获取分析后的分词。代码清单 5-3 展示了一个分析 `API` 的例子，它使用标准分析器分析了文本 `share your experience with NoSql & big data technologies`。

::: warning 代码清单 5-3 使用分析 API 的例子

```http
GET /_analyze
{
  "analyzer" : "standard",
  "text" : "share your experience with NoSql & big data technologies"
}
{
  "tokens" : [
    {
      "token" : "share",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "your",
      "start_offset" : 6,
      "end_offset" : 10,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "experience",
      "start_offset" : 11,
      "end_offset" : 21,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "with",
      "start_offset" : 22,
      "end_offset" : 26,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "nosql",
      "start_offset" : 27,
      "end_offset" : 32,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "big",
      "start_offset" : 35,
      "end_offset" : 38,
      "type" : "<ALPHANUM>",
      "position" : 5
    },
    {
      "token" : "data",
      "start_offset" : 39,
      "end_offset" : 43,
      "type" : "<ALPHANUM>",
      "position" : 6
    },
    {
      "token" : "technologies",
      "start_offset" : 44,
      "end_offset" : 56,
      "type" : "<ALPHANUM>",
      "position" : 7
    }
  ]
}
```

:::

分析 `API` 中最为重要的输出是 `token` 键。输出是一组这样映射的列表，代表了处理后的分词(实际上，就是这些分词将会被写入到索引中)。例如，输入文本`share your experience with NoSql & big data technologies`后，将获得 8 个分词，即 `share`、`your`、`experience`、`with`、`nosql`、`big`、`data` 和 `technologies`。请注意，这个案例使用了标准的分析器，这个分词被转换为小写，每个句子结尾的标点符号也被去除。这是一个非常棒的测试文档的方法，看看 `Elasticsearch` 是如何分析它们的，而且 `API` 还有数个方法可以定制运用于文本上的分析步骤。

## 选择一个分析器

如果你心中已经设想了一个分析器，而且像看看它是如何处理文本的，那么可以将 `analyzer` 参数设置为这个分析器的名字。下一节将逐个介绍不同的内置分析器，如果想尝试所有的这些，请务必记住这个操作！

如果在 `elasticsearch.yml` 文件中配置了一个分析器，你也可以通过 `analyzer` 参数中的名字来指向它。另外，如果已经使用类似代码清单 5-2 的定制分析器创建一个索引，你仍然可以通过名字来使用这个分析器，但是不再是使用 `HTTP` 的 `/_search` 端点，而是需要首先指定索引。下面展示了使用名为 `get-together` 的索引和名为 `myCustomAnalyzer` 的分析器的一个例子：

```http
POST /get-together/_analyze
{
  "analyzer": "myCustomAnalyzer",
  "text" : "share your experience with NoSql & big data technologies"
}
```

## 通过组合即兴地创建分析器

有的时候，你可能不想使用内置分析器，而是尝试分词和分词过滤器的组合。例如，在没有让任何其他分析步骤的情况下，看看一个特定的分词器如何切分句子。有了分析的 `API`，就可以指定一个分词器和一组分词过滤器，用于文本的分析。例如，如果想使用空白分词器(按照空白来切分文本)，然后使用小写和反转分词过滤器，可以这样做：

```http
POST /get-together/_analyze
{
  "tokenizer": "whitespace",
  "filter": ["lowercase","reverse"],
  "text" : "share your experience with NoSql & big data technologies"
}
```

将获得如下的分词：

```
erahs，ruoy，ecneirepxe，htiw，lqson，&，gib，atad，seigolonhcet
```

分词器首先将句子`share your experience with NoSql & big data technologies`切分为 `share`、`your`、`experience`、`with`、`NoSql`、`&`、`big`、`data` 和 `technologies`。接下来，它将这些分词转换为小写，最后将每个分词反转过来，获得结果词条。

## 基于某个字段映射的分析

一旦开始创建索引的映射，分析 `API` 也是很有价值的，这是因为 `Elasticsearch` 允许你基于所创建的映射字段来进行分析。如果像下面的代码片段这样创建了 `description`  字段的映射：

```json
"description": {
	"type": "text",
	"analyzer": "myCustomAnalyzer"
}
```

然后就可以通过指定请求中的 `field` 参数来使用和字段关联的分析器：

```http
POST /myindex/_analyze
{
  "field": "description",
  "text" : "share your experience with NoSql & big data technologies"
}
```

由于定制的分析与 `description` 字段进行了关联，它会被自动地运用在文本分析上。请记住为了使用这个特性，需要指定一个索引，原因是 `Elasticsearch` 需要从索引中获取特定字段的映射。

现在已经讨论了如何使用 `cURL` 来测试不同的分析器，接下来我们将深入 `Elasticsearch` 提供的各种不同分析器。请记住，总是可以组合不同的模块(分词器和分词过滤器)来创建自己的分析器。

## 使用词条向量 API 来学习索引词条

当考虑合适的分析器时，前一节的 `_analyze`端点是个很好的方法。但是对于特定文档中的词条，如果想学习更多内容，就存在一个比遍历所有单独字段更有效的方法。可以使用 `_termvectors` 的端点来获取词条的更多信息。使用这个端点就可以了解词条，了解它们在文档中、索引中出现的频率，以及出现在文档中的位置。

端点 `_termvectors` 的基本使用方法是这样的：

```
GET /get-together/_termvectors/1
{
  "_index" : "get-together",
  "_id" : "1",
  "_version" : 1,
  "found" : true,
  "took" : 4,
  "term_vectors" : {
    "description" : {//返回词条信息
      "field_statistics" : {
        "sum_doc_freq" : 269,//该字段中所有词条的文档频率之和
        "doc_count" : 16,//包含这个字段的文档数量
        "sum_ttf" : 287//字段中所有词条频率之和。如果一个词条在一个文档中出现多次，那么一定会大于0
      },
      "terms" : {//包含字段 description 中所有词条的对象
        "about" : {//属于结果数据的词条
          "term_freq" : 1,//词条在这个字段中出现的次数
          "tokens" : [//词条在字段中
            {
              "position" : 16,
              "start_offset" : 90,
              "end_offset" : 95
            }
          ]
        },
        "and" : {
          "term_freq" : 1,
          "tokens" : [
            {
              "position" : 13,
              "start_offset" : 75,
              "end_offset" : 78
            }
          ]
        },
        "clojure" : {
          "term_freq" : 2,
          "tokens" : [
            {
              "position" : 2,
              "start_offset" : 9,
              "end_offset" : 16
            },
            {
              "position" : 17,
              "start_offset" : 96,
              "end_offset" : 103
            }
          ]
        },
      }
    }
  }
}
```

还有些东西是可以配置的，其中之一是词条统计数据，请注意这个操作消耗很大。下面的命令展示了如何修改请求。现在，指定了需要统计数据的字段：

```http
GET /get-together/_termvectors/1
{
  "fields": ["description","tags"],
  "term_statistics": true
}
```

下面是响应的一部分，只显示了一个词条，结构和之前的代码样例相同。

```json
"about" : {//展示信息的词条
	"doc_freq" : 5,//出现这个词条的文档数
	"ttf" : 5,//索引中该词条的总词频
	"term_freq" : 1,
	"tokens" : [
		{
			"position" : 16,
			"start_offset" : 90,
			"end_offset" : 95
		}
	]
}
```

现在，已经学习了很多分析器的工作内容，以及如何阅读分析器的结果。在下一节中探索内置分析器的时候，将会持续使用 `_analyze` 和 `_termvectors` `API`。

