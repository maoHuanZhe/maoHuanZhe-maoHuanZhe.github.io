---
title: 监控瓶颈
date: 2022-04-19 20:54:21
permalink: /pages/a4c880/
categories:
  - Elasticsearch
tags:
  - Elasticsearch
  - 管理集群
  - 监控
author: 
  name: 樊光瑞
  link: https://github.com/maoHuanZhe
---
`Elasticsearch` 通过它的 `API` 接口提供了丰富的信息：内存消耗、节点成员、分片分发以及 `I/O` 的性能。集群和节点 `API` 测量集群的健康状况和整体的性能指标。理解集群的诊断数据并预估集群的整体状态，将提醒用户注意性能的瓶颈，如未分配的分片和消失的节点，这样就可以轻而易举地解决这些问题。
## 检查集群的健康状态
集群的健康 `API` 接口提供了一个方便但略有粗糙的概览，包括集群、索引和分片的整体健康状况。这通常是发现和诊断集群中常见问题的第一步。代码清单 11-3 展示了如何使用集群健康 `API` 来检查整体的集群状态。
::: warning 代码清单 11-3 集群健康 API 的请求
``` http
GET /_cluster/health
{
  "cluster_name" : "FGRAPP",
  "status" : "green",//集群状态指示器:方便的集群整体健康指示器
  "timed_out" : false,
  "number_of_nodes" : 3,//集群中节点的总数量
  "number_of_data_nodes" : 3,//集群中存放数据的节点总数量
  "active_primary_shards" : 25,//集群中全部索引的主分片总数量
  "active_shards" : 50,//集群中全部索引的所有分片、包括主分片和副本分片的总数量
  "relocating_shards" : 0,//当下正在多个节点间移动的分片数量
  "initializing_shards" : 0,//新创建的分片数量
  "unassigned_shards" : 0,//集群中定义、却未能发现的分片数量
  "delayed_unassigned_shards" : 0,//分配因超时设置而延迟的分片数量。
  "number_of_pending_tasks" : 0,//尚未执行的集群级更改的数量
  "number_of_in_flight_fetch" : 0,//未完成的fetches的数量
  "task_max_waiting_in_queue_millis" : 0,//自最早启动任务以来以毫秒为单位表示的时间正在等待执行
  "active_shards_percent_as_number" : 100.0//集群中活性分片的比例以百分比表示。
}
```
:::
从这个答复的表明信息，用户就可以推断出很多关于集群整体健康状态的信息，不过除了第一眼能看出的明显内容，还有很多可以解读的地方。让我们深入理解一下代码中 3 个指示器的含义：`relocating_shards`、`initializing_shards` 和 `unassigned_shards` 。
- `relocating_shards`——大于 0 表示 `Elasticsearch` 正在集群内移动数据的分片，来提升负载均衡和故障转移。这通常发生在添加新节点、重启失效的节点或者删除节点的时候，因此出现了这种临时的现象。
- `initializing_shards`——当用户刚刚创建一个新的索引或者重启一个节点的时候，这个数值会大于 0。
- `unassigned_shards`——这个值大于 0 的最常见原因是有尚未分配的副本分片。在开发环境中，这个问题很普遍，因为单节点的集群其索引默认有 5 个分片和 1 个副本分片。这种情况下，由于无多余节点来分配副本分片，因此还有 5 个未分配的副本分片。
从输出结果的第一行可以看出，集群的状态是绿色的。有时也不一定如此，如节点无法启动或者从集群中掉出的情况。尽管状态值只是给你一个大致的集群健康状态，理解状态值对于集群性能的含义还是很有价值的。
- 绿色——主分片和副本分片都已经分发而且运作正常。
- 黄色——通常这是副本分片丢失的信号。这个时候，`unassigned_shards` 的值很可能大于 0，使得集群的分布式本质不够稳定。进一步的分片损坏将导致关键数据的缺失。查看任何没有正确地初始化或运作的节点。
- 红色——这是危险的状态，无法找到集群中的主分片，使得主分片上的索引操作不能进行，而且导致了不一致的查询结果。同样，很可能一个或多个节点从集群中消失。
有了这些知识，用户现在可以查看一个黄色状态的集群，并试图跟踪问题的根源。
```
GET /_cluster/health
{
  "cluster_name" : "FGRAPP-test",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 1,
  "number_of_data_nodes" : 1,
  "active_primary_shards" : 10,
  "active_shards" : 10,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 2,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 83.33333333333334
}
```
给定这个 `API` 请求及其返回结果，用户可以看到目前集群处于黄色状态，根据之前所学，可能的罪魁祸首是 `unassigned_shards` 值大于 0 了。集群健康 `API` 提供了更多的细粒度的操作，允许用户进一步地诊断问题。在这个例子中，可以通过添加 `level` 参数，深入了解哪些索引受到了分片未配置的影响。
``` http
GET /_cluster/health?level=indices
{
 "cluster_name" : "FGRAPP-test",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 1,
  "number_of_data_nodes" : 1,
  "active_primary_shards" : 10,
  "active_shards" : 10,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 2,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 83.33333333333334,
  "indices" : {
    "index-test-yellow" : {
      "status" : "yellow",
      "number_of_shards" : 2,
      "number_of_replicas" : 1,
      "active_primary_shards" : 2,
      "active_shards" : 2,
      "relocating_shards" : 0,
      "initializing_shards" : 0,
      "unassigned_shards" : 2
    }
  }
}
```
单节点集群遭遇了一些问题，原因是 `Elastiesearch` 试图在集群内分配副本分片，但是由于只有一个节点在运行，它无法进行下去。这导致了副本分片未能分发到各处，所以集群的状态是黄色的，如图 11-3 所示。
正如你所见，一个简单的弥补方式是向集群中加入节点，这样 `Elasticsearch` 就能将副本分片配置到这个位置。请确保所有的节点都在运行而且可访问，这是解决黄色状态问题最简单的方法。
::: center 
![Elasticsearch](https://cdn.jsdelivr.net/gh/maoHuanZhe/image@main/20220424/Elasticsearch.6g1wdhofqh34.webp)
![Elasticsearch](https://cdn.jsdelivr.net/gh/maoHuanZhe/image@main/20220424/Elasticsearch.1owwovxl7uxs.webp)
图 11-3 通过更多的可访问节点，让黄色状态的问题得以解决
:::
## CPU：慢日志、热线程和线程池
监控 `Elasticsearch` 集群可能会不时地暴露 `CPU` 使用的突发高峰，或者是持续高 `CPU` 使用率、阻塞/等待线程导致的性能瓶颈。本节将阐明一些可能的性能瓶颈，并提供必要的工具让用户侦测并解决这些问题。
### 慢日志
`Elasticsearch` 提供了两项日志来区分慢操作，它们很容易在集群配置文件中设置：慢查询日志和慢索引日志。默认情况下两者都是关闭的。日志输出是分片级别的。因此在相应的日志文件中，一个操作可能由若干行表示。分片级别日志的好处在于，用户可以根据日志输出更好地定位有问题的分片和节点，如下所示。请注意这些设置可以通过 `(index_name/_settings` 端点进行修改。
``` http
PUT /index-test/_settings
{
  "index": {
    "search" :{
      "slowlog" :{
        "threshold": {
          "query": {
            "warn": "10s",
            "info": "1s",
            "debug": "2s",
            "trace": "500ms"
          },
          "fetch": {
            "warn": "1s",
            "info": "1s",
            "debug": "500s",
            "trace": "200ms"
          }
        }
      }
    }
  }
}
```
如你所见，可以为搜索的两个阶段设置阈值：查询和获取。日志的级别（警告-`war n`、信息-`info`、调试-`debug` 和跟踪-`trace`）允许用户细粒度地控制何种级别的内容要被记录下来，当想简单地查找(`grep`)日志文件时这一点很方便。在 `log4j2.properties` 文件中，可以配置存放日志输出的实际文件以及其他一些日志 功能，如下所示。
```

```
典型的慢日志文件输出看上去是这样的:
```

```
### 慢查询日志
对于识别性能问题，你会感兴趣的一个重要部分是查询的时间：`took[##ms]`。此外，了解相关的分片和索引也是很有帮助的，这些都是通过`[index][shard number]`符号来标识的，在这个例子中它是 [streamglue] [4]。
### 慢索引日志
在发现索引操作过程中的瓶颈时，同样有价值的是慢索引日志。它的阈值是在集群配置文件中定义，或者是通过索引更新设置的 `API ` 接口定义，和之前的慢日志相似
``` http
PUT /index-test/_settings
{
  "index": {
    "indexing" :{
      "slowlog" :{
        "threshold": {
          "index": {
            "warn": "10s",
            "info": "1ms",
            "debug": "2s",
            "trace": "500ms"
          }
        }
      }
    }
  }
}
```
和之前一样，任何达到阈值的索引操作将被写入到日志文件，而且用户将看到索引操作的索引名和分配数 `[Iindex] Ishard number] ( rbitbucket][2]）`和持续时间(`took [4 .5ms]`)。
```

```
发现慢查询和慢索引请求出现在哪里，对于纠正 `Elasticsearch` 的性能问题大有帮助。容忍性能低下的行为无限制地增长，可能会导致整个集群中的失败逐渐累积，最终集群完全崩溃。
### 热线程 API接口
如果遇到过集群的 `CPU` 使用率居高不下，则将发现热线程(`hot threads`) `API` 对于识别被阻塞并导致问题的具体进程，是很有用处的。这里热线程 `API` 提供了集群中每个节点上的一系列阻寨线程。请注意，和其他 `API` 接口不同，热线程并不返回 `JSON` 格式的内容，而是如下格式化的文本：
```
GET /_nodes/hot_threads
```
下面是输出的样例：
```
::: {node1}{-_RCA5-dRVKpG6fcQ5l9sw}{UsPlHiT0RrCHhhX2rTWrvA}{fgrapp1}{82.157.68.63:9300}{cdfhilmrstw}{ml.machine_memory=3917721600, xpack.installed=true, ml.max_jvm_size=536870912}
   Hot threads at 2022-04-24T09:08:43.790Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:
   
   100.0% [cpu=1.0%, other=99.0%] (500ms out of 500ms) cpu usage by thread 'elasticsearch[node1][search][T#6]'
     3/10 snapshots sharing following 23 elements
```
为了正确地理解，热线程 `API` 的输出需要一些解析，让我们看看它提供了哪些关于 `CPU ` 性能的信息：
```
::: {node1}{-_RCA5-dRVKpG6fcQ5l9sw}{UsPlHiT0RrCHhhX2rTWrvA}{fgrapp1}{82.157.68.63:9300}{cdfhilmrstw}{ml.machine_memory=3917721600, xpack.installed=true, ml.max_jvm_size=536870912}
```
结果的第一行包括了节点的身份。因为集群很可能有多于一个的节点，这是线程信息属于哪个 `CPU` 的第一个标识。
```
 100.0% [cpu=1.0%, other=99.0%] (500ms out of 500ms) cpu usage by thread 'elasticsearch[node1][search][T#6]'
```
这里可以看到 `100.0%` 的CPU 处理花费在了搜索（`search`）线程上。这对于你的理解很关键，因为可以调优导致 `CPU` 高峰的搜索查询。希望这里不要总是出现搜索。`Elasticsearch ` 可能显示其他的值，如 `merge`、`index`，来表明此线程进行的操作。由于 `cpu usage` 的标识，你知道了这是和 `CPU` 相关的。其他可能的输出标识有阳塞占比 `block usage`. 表示线程被阻塞了，以及等待占比 `wait usage` 表示线程在等待状态。
```
3/10 snapshots sharing following 23 elements
```
在堆栈轨迹(`stack trace`)之前的最后一行告诉你，`Elasticsearch` 在几毫秘中进行了 10 次快照，然后发现拥有如下同样堆栈轨迹的线程在这 3 次中都出现了。
当然，`Elasticsearch` 如何收集热线程 `API` 提供的信息，是值得学习的。每过几个毫秒，`Elasticsearch` 就会收集每个线程的持续时间、状态(等待阻塞)等待持续时间以及阻塞持续时间等相关的信息。过了指定的时间间隔(默认是 500 毫秒)，`Elasticsearch` 会对同样的信息进行第二轮收集操作。在每次收集过程中，它会对每个堆栈轨迹拍摄快照。用户可以通过向热线程 `API` 请求添加参数，来调整信息收集的过程：
```
GET /_nodes/hot_threads?type=wait&interval=1000ms&threads=3
```
- `type`——`cpu`、`wait` 和 `block` 之一。需要快照的线程状态类型。
- `interval`——第一次和第二次检查之间的等待时间。默认是 500 毫秒。
- threads——排名靠前的“热”线程的展示数量。 
### 线程池
集群中的每个节点通过线程池来管理 `CPU` 和内存的使用。`Elasticsearch` 将试图使用线程池以获得更好的节点性能。在某些情况下，需要手动地配置并修改线程池管理的方式，来避免失败累积（雪崩效应）的场景。在负载很重的情况下，`Elasticsearch` 可能会解化出上千个线程来处理请求，导致集群宕机。想要理解如何调优线程池，就需要精通应用程序使用 `Elasticsearch API` 的知识。举例来说，对于一个主要使用 `bulk` 索引 `API` 的应用程序，我们需要为其分配较多的线程。否则，`bulk index` 请求就会变得负载很重，而新进来的请求就会被忽略。
可以在集群配置中调节线程池的设置。线程池按照操作进行划分，并根据操作的类型配置默认值。为了简洁起见，这里只列出其中少数几个。
- `bulk`——默认是固定的值，基于可用于所有 `bulk` 批量操作的处理器数量。
- `index`——默认是周定的值，基于可用于索引和删除操作的处理器数量。
- `search`——默认是固定的值，3倍可用于计数和搜索操作的处理器数量。
查阅 `elasticsearch.yml` 配置，可以看到能为所有的批量操作增加线程池队列的大小，以及线程池的数量。**现有版本不再支持使用 API 动态设置。**
有两种线程池的类型，`fixed` 和 `scaling`。 `fixed` 的线程池类型保持固定数量的线程来处理请求，并使用后援队列来处理等待执行的请求。在这个例子中，`queue_size` 参数控制了线程的数量，默认是 `CPU` 核数的 5 倍。而 `scaling` 线程池类型是没有限制的，这意味着只要有任何等待执行的请求，系统就会创建一个新的线程。
``` yaml
//fixed 类型
thread_pool:
    write:
        size: 12
        queue_size: 1000
//scaling 类型
thread_pool:
    warmer:
        core: 1
        max: 8
        keep_alive: 2m
```
有了集群健康的 `API` 接口，慢查询和索引日志，以及线程信息，就可以更方便地诊断 `CPU` 集中的操作和瓶颈。下一节将讨论内存相关的信息，这些将有助于诊断和调优 `Elasticsearch` 的性能问题。
::: node 更新内容
[官方文档](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html)
:::
## 内存：堆的大小、字段和过滤器缓存
本节将会探索 `Elasticsearch` 集群中有效的内存管理和调优。很多 `Elasticsearch` 的聚集和过滤操作都是受限于内存的，所以理解如何有效地改善 `Elastiesearch` 默认的内存管理设置，以及底层的 `JVM` 虚拟机对于扩展集群非常有用。
### 堆的大小
`Elasticsearch` 是运行于`Java` 虚拟机（`JVM`）之上的 `Java` 应用程序，所以它受限于垃圾回收器(`garbage collector`)的内存管理。垃圾回收器的基本概念是非常简单的．当空闲内存不够用的时候，就会触发垃圾回收，清理已经不再引用的对象，以此来释放内存供其他 JVM 应用程序使用。这些垃圾回收操作是很耗时间的，并会引起系统的停顿。将过多的数据加载到内存也会导致 `OutOfMemory` 的异常，引起失败和不可预测的结果——甚至是垃圾回收器都无法解决的问题。
为了让 `Elasticsearch` 更快，某些操作在内存中执行，因为字段数据的读取已被优化。例如，`Elasticsearch` 不仅加载和查询匹配的文档之字段数据，它还加载了索引中全部文档的值。通过快速访问内存中的数据，后续的查询会快得多。
`JVM` 堆表示了分配给 `JVM` 上运行的应用程序之内存量。由于这个原因，理解如何调优其性能来避免垃圾收集停顿带来的副作用和 `OutOfMemory` 异常，是非常重要的。用户可以通过 `HEAP_SIZE` 环境变量来设置 `JVM` 堆的大小。当设置堆的大小时，请牢记两条如下的黄金法则。
- 最多 50% 可用的系统内存——分配过多的系统内存给 `JVM` 就意味着分配给底层文件系统缓存的内存更少，而文件系统缓存却是 `Lucene` 需要经常使用的。
- 最大 `32G` 内存——分配了超过 32GB 的内存之后，`JVM` 的行为就会发生变化，不再使用压缩的普通对象指针(`OOP `)。这就意味着堆的大小少于 32GB 时，只需要使用大约一半的内存空间。
### 过滤器和字段缓存
缓存对于 `Elasticsearch` 的性能而言，扮演着非常重要的角色。它允许用户有效地使用过滤器、切面（`facet`）和索引字段的排序。本节将探索两种缓存：过滤器缓存和字段数据缓存。
过滤器缓存将过滤器和查询操作的结果存放在内存中。这意味着使用过滤器的初始查询将其结果存储在过滤器缓存中。随后应用该过滤器的每次查询都会使用缓存中的数据，而不会去磁盘查找。过滤器缓存有效地降低了对 `CPU` 和 `I/O` 的影响，并使得过滤查询的结果返回更加迅速。
节点查询缓存采用的是 `LRU`（近期最少使用）缓存类型。这意味着当缓存要满的时候，使用次数最少的缓存条目将被首先销毁，为新的条目腾出空间。可以使用 `indices.queries.cache.size` 属性来设置缓存大小。大小值是在节点的 `elasticsearch.yml` 配置中定义的，它既可以使用内存的百分比（`20%`），也可以使用静态的值（`1024 MB` 字节）。请注意，百分比的属性是将节点上最大的堆内存作为,总数来进行计算的。
::: node 更新内容
[官方文档](https://www.elastic.co/guide/en/elasticsearch/reference/8.1/query-cache.html)
:::
### 字段数据缓存
字段数据缓存用于提升查询的执行时间。当运行查询的时候，`Elasticsearch` 将字段值加载到内存中并将它们保存在字段数据的缓存中，用于之后的请求。由于在内存中构建这样的结构是成本很高的操作，你不希望 `Elasticscarch` 对于每次请求都执行这个动作，这样性能的提升才会明显。默认地，这是一个没有限制的缓存，也就是说它会持续增长，直到触动了字段数据的断路器（下一节会讨论)。通过为字段数据缓存设置上限值，你告诉 `Elasticsearch` 一旦达到这个上限，就将数据从缓存结构中移除
你的配置应该包含了一个 `indices.fielddata.cache.size` 属性，它既可以设置为百分比（20%）也可以设置为静态的值（`16GB`)。这些值表示了用于缓存的节点堆内存空间之百分比或绝对值。
为了获取字段数据缓存的现有状态，你可以使用一些方便的 `API` 接口。
- 按照每个节点来看：
``` http
GET /_nodes/stats/indices/fielddata?fields=field*
```
- 按照每个索引来看：
``` http
GET /_stats/fielddata?fields=field*

```
- 按照每个节点的每个索引来看：
``` http
GET /_nodes/stats/indices/fielddata?fields=field*&level=indices
```
设置 `fields=*` 将返回所有的字段名称和取值。这些 `API` 接口的输出和下面的类似。
``` json
      "indices" : {
        "fielddata" : {
          "memory_size_in_bytes" : 688,
          "evictions" : 0,
          "fields" : { }
        }
}
```
这些操作将分析缓存的现有状态。请特别注意 `evictions`（移除数据）的次数。数据的移除是成本昂贵的操作，其发生表明字段数据的缓存规模可能设置得过小。
### 断路器
在之前的章节我们提到，字段数据缓存可能会不断增加最终导致 `OutOfMemory` 的异常。这是因为字段数据的规模是在数据加载之后才计算的。为了避免这种情况的发生，`Elasticsearch` 提供了断路器的机制。
断路器是人为的限制，用来帮助降低 `OutOfMemory` 异常出现的概率。它们通过反省某个查询所请求的数据字段，来确定将这此数据加载到缓存后是否会使得整体的规模超出缓存的大小限制。在 `Elasticsearch` 中有两个断路器，还有一个是父辈断路器，它在所有断路器可能使用的内存总量上又设置了一个限制。
- `indices.breaker.total.limit`——默认是堆内存的70%。不允许字段数据和请求断路器超越这个限制。
- indices.breaker.fielddata.limit——默认是堆内存的 60%。不允许字段数据的缓存超越这个限制。
- `indices.breaker.request.limit`——默认是堆内存的 40%。控制分配给聚集桶创建这种操作的堆大小。
断路器设置的黄金法则是对其数值的设定要保守，因为断路器所控制的缓存需要和内存缓冲区、过滤器缓存和其他 `Elasticsearch` 内存开销一起共用内存空间。
### 避免交换
操作系统通过交换（`swap`）进程将内存的分页写入磁盘。当内存的容量不够操作系统使用的时候，这个过程就会发生。当操作系统需要已经被交换出去的分页时，这些分页将被再次加载回内存以供使用。交换是成本高昂的操作，应该尽量避免。
`Elasticsearch` 在内存中保留了很多运行时必需的数据和缓存，如图 11-4 所示，所以消耗资源的磁盘读写操作将严重地影响正在运行的集群。鉴于这个原因，我们将展示如何关闭交换以获得更好的性能。
::: center
![Elasticsearch](https://cdn.jsdelivr.net/gh/maoHuanZhe/image@main/20220425/Elasticsearch.72drc7yics5c.webp)
图 11-4 Elasticsearch 将运行时的数据和缓存都放在内存中，因此读写操作可能是很昂贵的
:::
关闭 `Elasticsearch` 交换最彻底的方法是，在 `elasticsearch.yml` 文件中将 `bootstrap.memory_lock` 设置为 `true`。接下来，需要验证设置是否生效。运行 `Elasticsearch `，可以检查警告日志或者是查询一个活动状态。
- 日志中的错误样例：
```
Unable to lock JVM Memory
```
- API 请求：
``` http
GET /_nodes/process
```
- 请求回复：
``` json
      "process" : {
        "refresh_interval_in_millis" : 1000,
        "id" : 78502,
        "mlockall" : false
      }
```
如果在日志中发现了如此的警告信息，或者是状态检查的结果中 `mlockall` 被设置为了 `false`，那么你的设置还未生效。运行 `Elasticsearch` 的用户没有足够的访问权限，是新设置没有生效的最常见原因。通常以 `root` 用户的身份在命令行运行 `ulimit -l unlimited` 可以解决这个问题。为了这些新设置能生效，还需要重启 `Elasticsearch` 。
## 操作系统缓存
由于 `Lucene` 不可变的分段，`Elasticsearch` 和 `Lucene` 很大程度上使用了操作系统的文件缓存。按照设计，`Lucene` 利用底层的操作系统文件缓存来构建内存里的数据结构。`Lucene` 分段存储于单个不可变的文件中。大家认为不可变的文件对于缓存而言是友好的，而底层的操作系统将“热门”的分段保存于内存中，以便更快地访问。最终的效果就是，更小的索引更容易于被操作系统完全缓存于内存中，无须读取磁盘而且超快。
由于 `Lucene` 很大程度上使用操作系统的文件缓存，之前我们已经建议将 `JVM` 堆的大小设置为物理内存的一半，这样 `Lucene` 就能使用剩下的一半作为缓存。出于这一点，最佳实践会将经常使用的索引存放在更快的机器上。其基本思路是，`Lucene` 将热门的数据分段保留在内存中以便超快地读取，而对于有更多非堆内存的机器而言这一点更容易办到。不过，为了实现这个目标，需要使用路由将具体的索引分配到更快的节点上。
首先，需要为全部的节点分配一个特定的属性 `tag`。每个节点的 `tag` 值是唯一的，如 `node.attr.tag: mynode1` 或者 `node.attr.tag: mynode2`。使用节点的单独设置，可以只在拥有指定 `tag` 值的节点上创建索引。请记住，这样做的意义在于确保新的、繁忙的索引只会在拥有更多非堆内存的节点上创建，`Lucene` 也可以充分利用这些内存。为了达到这个目的，使用下面的命令，这样新索引 `myindex` 只会在 `tag` 值为 `mynode1` 和 `mynode2` 的节点上创建。
```
PUT myindex/_settings
{
  "index.routing.allocation.include.tag": "mynode1,mynode2"
}
```
假设这些特定的节点有更多的非堆内存可分配，`Lucene` 将在内存中缓存分段，使得你的索引拥有比查找磁盘分段更短的响应时间。
::: node 官网地址
[字段数据缓存](https://www.elastic.co/guide/en/elasticsearch/reference/8.1/modules-fielddata.html)
[断路器设置](https://www.elastic.co/guide/en/elasticsearch/reference/8.1/circuit-breaker.html#fielddata-circuit-breaker)
[避免交换](https://www.elastic.co/guide/en/elasticsearch/reference/8.1/setup-configuration-memory.html)
[索引级别分片分配过滤](https://www.elastic.co/guide/en/elasticsearch/reference/current/shard-allocation-filtering.html)
[存储限流已被删除](https://www.elastic.co/guide/en/elasticsearch/reference/2.0/breaking_20_setting_changes.html#_merge_and_merge_throttling_settings)
:::