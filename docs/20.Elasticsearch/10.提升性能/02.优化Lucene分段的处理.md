---
title: 优化Lucene分段的处理
date: 2022-04-19 20:51:16
permalink: /pages/5430f8/
categories:
  - Elasticsearch
  - 提升性能
tags:
  - refresh
  - flush
author: 
  name: 樊光瑞
  link: https://github.com/maoHuanZhe
---
一旦 `Elasticsearch` 接收到了应用所发送的文档，它会将其索引到内存中称为分段(`segments`)的倒排索引。这些分段会不时地写入磁盘。第 3 章曾经提到过，这些分段是不能政变的，只能被删除，这是为了操作系统更好地缓存它们。另外，较大的分段会定期从较小的分段创建而来，用于优化倒排索引，使搜索更快。
有很多调节的方式来影响每一个环节中 `Elasticsearch` 对于这些分段的处理，根据你的使用场景来配置这些，常常会带来意义重大的性能提升。本章将讨论这些调优的方式，可以将它们分为以下 3 类。
- 刷新(`refresh`)和冲刷(`flush`)的频率——刷新会让 `Elasticsearch` 重新打开索引，让新建的文档可用于搜索。冲刷是将索引的数据从内存写入磁盘。从性能的角度来看，刷新和冲刷操作都是非常消耗资源的，所以为你的应用正确地配置它们是十分重要的。
- 合并的策咯——`Lucene`(`Elasticsearch` 也是如此)将数据存储在不可变的一组文件中，也就是分段中。随着索引的数据越来越多，系统会创建更多的分段。由于在过多的分段中搜索是很慢的，因此在后台小分段会被合并为较大的分段，保持分段的数量可控。不过，合并也是十分消耗性能的，对于 `I/O` 子系统尤其如此。你可以调节合并的策略，来确定合并多久发生一次，而且分段应该合并到多大。
- 存储和存储限流——`Elasticsearch` 调节每秒写入的字节数，来限制合并对于 `I/O` 系统的影响。根据硬件和应用，你可以调整这个限制。还有一此其他的选项告诉 `Elasticsearch` 如何使用存储。例如，可以选择只在内存中存放索引
这 3 个分类中，通常有一类会给你带来最大的性能收益，我们就从它开始：选择刷新和冲刷的频率
## 刷新和冲刷的阈值
还记得第二章的介绍吗？ `Elasticsearch` 通常被称为近实时（或准实时）系统。这是因为搜索不是经常运行于最新的索引数据之上(这个数据是实时的)，但是很接近了。
打上近实时的标签是因为通常 `Elasticsearch` 保持了某个时间点索引打开的快照，所以多个搜索会命中同一个文件并重用相同的缓存。在这个时间段中，新建的文档对于那些搜索是不可见的，直到你再次刷新。
刷新，正如其名，是在某个时间点刷新索引的快照，这样你的搜索就可以命中新索引的数据。这是其优点。其不足是每次刷新都会影响性能：某些缓存将失效，拖慢搜索请求，而且重新打开索引的过程本身也需要一些处理能力，拖慢了索引的建立。
### 何时刷新
默认的行为是每秒自动地刷新每份索引。你可以修改其设置，改变每份索引的刷新间隔，这个是可以在运行时完成的。例如，下面的命令将自动刷新的间隔设置为了 5 秒。
```
PUT /get-together/_settings
{
  "index.refresh_interval":"5s"
}
```
提示 为了确定你的修改生效了，可以运行如下命令来获得全部的索引设置：
```
GET /get-together/_settings
```
当增加 `refresh_interval` 的值时，你将获得更大的索引吞吐量，因为花费在刷新上的系统资源更少了。或者你也可以将 `refresh_interval` 设置为 -1，彻底关闭自动刷新并依赖手动刷新。这对于索引只是定期批量变化的应用非常有效，如产品和库存每晚更新的零售供应链。索引的吞吐量是非常重要的，因为你总想快速地进行更新，但是数据刷新不一定是最重要的，因为无论如何都不可能获得完全实时的更新。所以每晚你可以关闭自动刷新，进行批量的 `bulk`索引和更新。完成后再进行手动刷新。
为了实现手动刷新，访问待刷新索引的 `_refresh` 端点。
```
POST /get-together/_refresh
```
::: node 注意
未刷新的数据可以通过指定 `ID` 获取，但是不会被搜索到
:::
### 何时冲刷
如果你习惯了老版本的 `Lucene` 或者 `Solr`，可能会倾向于认为，当刷新发生的时候，所有的数据(内存中的)都已经完成了索引，因为最近一次的刷新也会将其写入磁盘。
对于 `Elasticsearch`(还有 4.0 及之后版本的 `Solr`)而言，刷新的过程和内存分段写入磁盘的过程是相互独立的。实际上，数据首先索引到内存中，经过一次刷新后，`Elasticsearch` 也会开心地搜索相应的内存分段。将内存中的分段提交到磁盘上的 `Lucene` 索引的过程，被称为冲刷(`fush`)，无论分段是否能被搜到，冲刷都会发生。
为了确保某个节点宕机或分片移动位置的时候，内存数据不会丢失，`Elasticsearch` 将使用事物日志来跟踪尚末冲刷的索引操作。除了将内存分段提交到磁盘，冲刷还会清理事物日志，
如图 10-2 所示。
::: center
![Elasticsearch](https://cdn.jsdelivr.net/gh/maoHuanZhe/image@main/20220426/Elasticsearch.6ao4eakjn0cg.webp)
图 10-2 冲刷操作将分段从内存中移动到磁盘上，并清除事务日志
:::
如图 10-3 所示，满足下列条件之一就会触发冲刷操作。
- 内存缓冲区已满。
- 自上次冲刷后超过了一定的时间。
- 事物日志达到了一定的阈值
::: center
![Elasticsearch](https://c dn.jsdelivr.net/gh/maoHuanZhe/image@main/20220426/Elasticsearch.jqc2wxgtqf4.webp)
图 10-3 内存缓冲区已满、事物日志已满、时间间隔已到，都会触发冲刷操作
:::
为了控制冲刷发生的频率，你需要调整控制这 3 个条件的设置。
内存缓冲区的大小在 `elastiescarch.yml` 配置文件中定义，通过 `indices.memory.index_buffer_size` 来设置。这个设置控制了整个节点的缓冲区，其值可以是全部 `JVM` 堆内存的百分比，如10%，也可以是 `100 MB` 这样的固定值。
事物日志的设置是具体到索引上的，而且同时控制了触动冲刷的规模(通过`index.translog.flush_threshold_size`)。和多数索引设置一样，你可以在运行时修改它们。
```
PUT /get-together/_settings
{
  "index.translog.flush_threshold_size": "500mb"
}
```
::: node 更新内容
`index.translog.flush_threshold_period` 参数已被删除
[持久化](https://cloud.tencent.com/developer/article/1765827)
:::
当冲刷发生的时候，它会在磁盘上创建一个或多个分段。执行一个查询的时候，`Elasticsearch`(通过 `Lucene`)查看所有的分段，然后将结果合并到一个整体的分片中。如你在第 2 章所见，搜索的时候每个分片上的结果将被聚集为一个完整的结果集合，然后返回给应用程序。
关于分段，这里需要记住的关键点是你需要搜索的分段越多，搜索的速度就越慢。为了防止分段的数量失去控制，`Elasticsearch`(也是通过 `Lucene`)在后台将多组较小的分段合并为较大的分段。
## 合并以及合并策略
在第 3 章中我们首次介绍分段，它是不变的一组文件，`Elasticsearch` 用其存储索引的数据。由于分段是不变的，它们很容易被缓存，使得搜索更快。此外，修改数据集时，如添加一篇文档，无须重建现有分段中的数据索引。这使得新文档的索引也是很快的，但也不都是好消息。更新文档不能修改实际的文档，只是索引一篇新的文档。如此处理还需要删除原有的文档。接下来，删除也不能从分段中移除文档(这需要重建倒排索号)，只是在单独的 `.del` 文件中将其标记为 “已被删除”。文档只会在分段合并的时候真正地被移除。
这告诉我们合并分段的两个目的：第一个是将分段的总数量保持在受控的范围内(这用来保障查询的性能)。第二个是真正地删除文档。
按照已定义的合并策略，分段是在后台进行的。默认的合并策略是分层配置，如图 10-4 所示，该策略将分段划分为多个层次，如果你的分段多于某一层中所设置的最大分段数，该层的合并就会被触发。
::: center
![Elasticsearch](https://cdn.jsdelivr.net/gh/maoHuanZhe/image@main/20220427/Elasticsearch.3zskbeahlce8.webp)
图 10-4 当分层的合并策略发现某层中存在过多的分段时，它将进行一次合并
:::
还有其他的合并策略，但是在这章中我们只会聚焦于默认的分层合并策略，原因是多数情况下这个策略能起到很好的效果。
### 调优合并策略的选项
合并的最终目的是提升搜索的性能而均衡 `I/O` 和 `CPU` 计算能力。合并发生在索引、更新或者删除文档的时候，所以合并的越多、这些操作的成本就越昂贵。反之，如果想快速地索引，你需要较少的合并，而且牺牲一些查询的性能，如图 10-4 所示。
为了设置合并的多少，你有几个设置选项。这里列出最重要的几个。
- `index.merge.policy.segments_per_tier`——这个值越大，每层可以拥有的分段越多。这就意味着更少的合并以及更好的索引性能。如果索引次数不多，而你希望获得更好的搜索性能，将这个值设置的低一些。
- `index.merge.policy.max_merge_at_once`——这个设置限制了每次可以合并多少个分段。通常，你可以将其等同于 `segments_ per_tier value` 的值。可以降低 `max_merge_at_once` 的值来强制性地减少合并，但是最好是通过增加 `segments_ per_tier` 来实现这个目的。请确保 `max_merge_at_once` 的值不会比 `segments_ per_tier` 的值高，因为这会引起过多的合并。
- `index.merge.policy.max_merged_segment`——这个设置定义了最大的分段规模。不会再使用其他的分段合并为比这个更大的分段了。如果你想获得较少的合并次数，以及更快的索引速度，最好降低这个值，因为较大的分段更难以合并。
- `index.merge.scheduler.max_thread_count`——在后台，合并发生于多个彼此分隔的线程中，而这个设置控制了可用于合并的最大线程数量。这是每次可以进行的合并的硬性限制。在一台多 `CPU` 和高速 `I/O` 的机器上，你可以增加这个设置来实行激进的合并策略，在低速 `CPU` 和 `I/O` 的机器上需要降低这个值。
所有这些选项是具体到索引上的，而且和事物日志及刷新设置一样，你可以在运行时修改这些设置。例如，下面的代码片段将 `segments_per_tier` 减少到 5，会导致更多的合并(还有 `max_merge_at_once` )，将最大的分段规模降低到 `1GB`，并将线程数量降低到 1，让磁盘更好地运转。
```
PUT /get-together/_settings
{
  "index.merge": {
    "policy": {
      "segments_per_tier": 5,
      "max_merge_at_once": 5,
      "max_merged_segment": "1gb"
    },
    "scheduler.max_thread_count": 1
  }
}
```
### 优化索引
有了刷新和冲刷，你可以手动触发一次合并。一次强制性的合并也被称为优化(`optimize`)，之所以起这样的名宇是因为通常是在一个今后不会更改的索引上运行这个操作，将其优化到一定(较低)数量的分段，使得更快的搜索成为可能。
对于激进的合并而言，优化是非常消耗 `I/O` 的，而且使得许多缓存失效。如果你持续地索引、更新和删除索引文件中的文档，新的分段就会被创建，而优化操作的好处就无法体现出来。因此，在一个不断变化的索引上，如果希望分段的数量较少，那么你应该调优合并的策略。
在静态的索引上优化是很有意义的。例如，如果索引了社会媒体的数据，而且每天新建一个索引，那么你知道自己永远不会修改昨天的索引，直到有一天永远删除它。这种情况下，将分段优化为较少的数量可能是很有帮助的，如图 10-5 所示。图 10-5 中，系统会减少分段的总数量，且缓存再次被预热加载，就会加速查询。
::: center
![Elasticsearch](https://cdn.jsdelivr.net/gh/maoHuanZhe/image@main/20220427/Elasticsearch.1ra2uz6xhsqo.webp)
图 10-5 对于没有更新的索引而言，优化操作是很有意义的
:::
为了优化，你需要访问待优化索引的 `_forcemerge` 端点。选项 `max_num_segments` 表示每
个分片最终拥有多少分段。
```
POST /get-together/_forcemerge?max_num_segments=1
```
在一个大型索引上进行的优化操作可能需要花费很长时间。
导致优化(或合并)操作缓慢的可能原因之一是，默认情况下 `Elastiesearch` 限制了合并操作所能使用的 `I/O` 吞吐量的份额。该限制称为存储限流(`store throttling`)。
::: node 更新内容
合并节流现在使用反馈循环来自动节流。这些设置已被删除：
```
indices.store.throttle.type
indices.store.throttle.max_bytes_per_sec
index.store.throttle.type
index.store.throttle.max_bytes_per_sec
```
[存储限流已被删除](https://www.elastic.co/guide/en/elasticsearch/reference/2.0/breaking_20_setting_changes.html#_merge_and_merge_throttling_settings)
:::
## 存储
### 配置存储
当我们讨论冲刷和合并的时候，提及了 `磁盘` 和 `I/O`，因为这是默认的选择：`Elasticsearch` 会将索引存储到数据目录，如果你是从 `RPM/DEB ` 包安装的 `Elasticsearch`，那么默认值是 `/var/lib/elasticsearch/data`，如果你是手动地解压 `tar.gz` 或 `ZIP` 压缩包来安装的，那么默认目录是 `data/`。可以通过 `elasticsearch.yml` 的 `path.data` 属性来修改数据目录。
::: tips 提示 
在 `path.data` 属性中，你可以指定多个目录(至少在版本1.5中是可以的)，这会将不同的文件放入不同的目录，以此获得数据的拆分(假设这些目录在不同的磁盘上)。如果那是你所追求的，一定要有足够的经费使用 `RAID0 ` 磁盘阵列，确保性能和可靠度都很不错。考虑到这一点，规划最好是将每个分片放入同样的目录，而不是将其拆分到不同的磁盘。
:::
默认的存储实现将索引文件存放到文件系统，多数情况下这没有什么问题。为了访问 `Lucene` 的分段文件，默认的存储实现使用了 `Lucene` 的 `MMapDirectory`，它通常用于大型的文件，或者是需要随机访问的文件，如词条字典。对于其他类型的文件，如存储字段，`Elasticsearch` 使用了 `Lucene` 的 `NIOFSDirectory`。
### MMapDirectory
`MMapDirectory` 利用了文件缓存，请求操作系统将所需的文件映射到虚拟内存，这样能更快地直接访问这些内存。对于 `Elasticsearch` 而言，看上去所有的文件都是可以在内存中访问的，但是实际情况不必如此。如果你的内存规模大于可用的物理内存，操作系统会将没有使用的文件移出缓存，为需要读取的新文件腾出空间。如果 `Elasticsearch` 再次需要哪些末被缓存的文件，这些文件会被加载到内存，而其他没有使用的文件被挪出内存等如此反复。`MMapDirectory` 使用的虚拟内存和系统的虚拟内存(交换空间)工作方式相似。对于系统的虛拟内存，操作系统将没有使用的内存空间放入磁盘，这样整个系统就能够服务于多个应用程序。
### NIOFSDirectory
内存映射的文件，也会导致额外的负载，因为应用程序必须要告诉操作系统在访问文件之前先对其进行映射。为了减小这个开销，`Elasticsearch` 为某些类型的文件使用了 `NIOFSDirectory`。`NIOFSDirectory` 是直接访问文件的，但是它必须将所需的数据复制到 `JVM` 堆的缓存中。对于小型的、按序访问的文件这样操作没有问题，而同时 `MMapDirectory` 能很好地工作于大型随机访问的文件。
对于多数情况，默认的存储实现就已经很棒了。但是，可以将索引设置中 `index.store.type` 的值修改为其他值来选择不同的实现。
- `mmapfs`——该选项只会使用 `MMapDirectory`。如果你有一个相对静止的索引，而且物 理内存也能放下该索引，那么这种选择也会运作得很好。
- `niofs`——该选项只会使用 `NIOFSDirectory`，在 32 位系统上可以很好地运作。因为在 32 位系统中，虚拟内存的寻址空间被限制在 4 GB。这种大小也使得用于更大索引的 `mmapfs` 选项不太适合。
- `hybridfs`—— `hybridfs` 类型是 `niofs` 和 `mmapfs` 的混合体，它根据读取访问模式为每种类型的文件选择最佳文件系统类型。目前，只有 `Lucene` 术语字典、规范和文档值文件进行内存映射。所有其他文件均使用 `Lucene` `NIOFSDirectory`打开。与`mmapfs`类似，请确保您允许大量的虚拟地址空间。
当创建索引的时候，需要设置存储类型。例如，下面的命令创建了一个基于 `mmap` 的索引，称为单元测试(`unit-test` )
```
PUT /unit-test
{
  "settings": {
    "index.store.type": "mmapfs"
  }
}
```
如果你想对所有新建的索引，都运用同样的存储类型，可以在 `elasticsearch.yml` 中将 `index.store.type` 设置为 `mmapfs`。第 11 章将介绍索引模板，它允许你为匹配特定模式的新索引定义索引的设置。模板可以在运行时进行修改，因此如果你经常创建新的索引，我们推荐使用模板而不是较为静态的 `elasticsearch.yml`。
::: tips 文件打开和虚拟内存的限制
存储于磁盘上的 `Lucene` 分段可以分布在多个文件中，当搜索运行时，操作系统需要打开很多文件。同样，当你使用默认的存储类型或 `mmapfs` 时，操作系统不得不将一些存储的文件映射为内存——即使这些文件实际上并不在内存中，应用程序还是认为它们位于内存里，而系统内核负责将这些文件加载到缓存和移出缓存。`Linux` 系统已经配置了一定的限制，防止应用程序一次性打开过多的文件而消耗过大的内存。对于 `Elasticsearch` 部署的需求而言，这些设置通常过于保守，所以我们建议增加这个设置。如果你是从 `DEB` 或 `RPM` 包安装的 `Elasticsearch`，不必担心这一点，因为默认情况下就会增加这个值。在可以在 `/etc/default/elasticsearch` 或 `/etc/sysconfig/elasticsearch` 中找到这些变量
```
MAX_OPEN_FILES=65535
MAX_MAP_COUNT=262144
```
为了手动增加这些限制值，需要以启动 `Elasticsearch` 的用户身份为打开文件运行 `ulimit -n 65535`，以 `root` 管理员的身份为虚拟内存运行 `sysctl -w vm.max_map_count =262144`。
由于操作系统缓存文件的方式，默认的存储类型通常是最快的。为了使缓存运作良好，需要足够的空闲内存。
::: tips 提示 
从版本2.0的 `Elasticsearch` 开始，你可以将 `index.codec` 设置为 `best_compression`，来压缩存储字段(以及`_source`)。默认的值(名为`fs`，存储类型自带的）仍然会使用 `LZ4` 算法来压缩存储字段，但是 `best_compression` 使用的是 `deflate` 压缩算法。更高的压缩率会拖慢需要 `_source` 的操作，如获取结果或者是关键词高亮。其他的操作，如聚集，应该是差不多快的，因为整体的索引规模会更小，也更容易缓存。
:::
我们曾经提到，`mergr` 操作是如何使缓存失效的。让 `Elasticsearch` 管理缓存并保持良好的性能，是值得好好地解释一番，接下来讨论相关的内容。
::: node 官方文档
[存储](https://www.elastic.co/guide/en/elasticsearch/reference/8.1/index-modules-store.html)
:::